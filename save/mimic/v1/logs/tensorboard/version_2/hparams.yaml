test: false
validate: false
dataset: chexpert
annotation: /workspace/CheXpert/Dataset/annotation.json
base_dir: /workspace/CheXpert/Dataset/PNG_chunk0_10GB_F
batch_size: 6
val_batch_size: 16
test_batch_size: 16
prefetch_factor: 4
num_workers: 8
vision_model: /workspace/CheXpert/Models/swin-base
llama_model: /workspace/CheXpert/Models/llama2-7b-chat
freeze_vm: true
llm_use_lora: false
llm_r: 16
llm_alpha: 16
vis_use_lora: false
vis_r: 16
vis_alpha: 16
lora_dropout: 0.1
global_only: false
low_resource: false
end_sym: </s>
savedmodel_path: save/mimic/v1
ckpt_file: null
delta_file: null
weights:
- 0.5
- 0.5
scorer_types:
- Bleu_4
- CIDEr
learning_rate: 0.0001
gradient_clip_val: null
beam_size: 3
do_sample: false
no_repeat_ngram_size: 2
num_beam_groups: 1
min_new_tokens: 80
max_new_tokens: 120
max_length: 100
repetition_penalty: 2.0
length_penalty: 2.0
diversity_penalty: 0
temperature: 0
devices: 1
num_nodes: 1
accelerator: gpu
strategy: ddp
precision: bf16-mixed
limit_val_batches: 1.0
limit_test_batches: 1.0
limit_train_batches: 1.0
max_epochs: 1
every_n_train_steps: 0
val_check_interval: 1.0
accumulate_grad_batches: 1
num_sanity_val_steps: 2
